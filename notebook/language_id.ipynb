{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Language Identifier Using Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Loading and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets merged and saved!\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\user\\Language-ID\\data\\Language_ID\\masakhanews\"\n",
    "processed_path = r\"C:\\Users\\user\\Language-ID\\data\\Language_ID\\masakhanews\\processed\"\n",
    "\n",
    "# target laguages\n",
    "\"\"\"\n",
    "'amh' for Amharic, 'ibo' for Igbo\n",
    "'orm' for Oromo, 'pcm' for Nigerian Pidgin\n",
    "'run' for Rundi, 'sna' for chiShona\n",
    "\"\"\"\n",
    "target_languages = [\"amh\", \"ibo\", \"orm\", \"pcm\", \"run\", \"sna\"]\n",
    "\n",
    "def load_and_merge(split):\n",
    "  \"\"\"\n",
    "  loads all datasets for a given split (train/test/val),\n",
    "  adds a 'lang' column, and merges them into a single df\n",
    "  \"\"\"\n",
    "  all_data = []\n",
    "\n",
    "  for lang in target_languages:\n",
    "    file_path = os.path.join(dataset_path, lang, f\"{split}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "      df = pd.read_csv(file_path)\n",
    "      df[\"lang\"] = lang\n",
    "      all_data.append(df)\n",
    "\n",
    "  return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "#load datasets\n",
    "train_data = load_and_merge(\"train\")\n",
    "test_data = load_and_merge(\"test\")\n",
    "val_data = load_and_merge(\"validation\")\n",
    "\n",
    "#save datasets to /processed\n",
    "train_data.to_csv(os.path.join(processed_path, \"train.csv\"), index=False)\n",
    "test_data.to_csv(os.path.join(processed_path, \"test.csv\"), index=False)\n",
    "val_data.to_csv(os.path.join(processed_path, \"validation.csv\"), index=False)\n",
    "\n",
    "print(\"Datasets merged and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                          headline  \\\n",
      "0      5  የስፖርት ኮከቦች እና የንግድ ምልክቶቻቸው- ከቦልት እስከ ክርስቲያኖ ሮናልዶ   \n",
      "1      5         እግር ኳስ፡ ዩናይትድ፣ አርሴናል፣ ቼልሲ . . . ምን አስበዋል?   \n",
      "2      0              ዓለምን ካስጨነቃት የዋጋ ንረት ተጠቃሚዎቹ እነማን ናቸው?   \n",
      "3      2         ኮሮናቫይረስ፡ በቫይረሱ የሞቱት የሮማኒያው ከንቲባ በምርጫ አሸነፉ   \n",
      "4      2       ኮሮናቫይረስ፡ አውሮፕላኖች እንዴት ነው በፀረ- ተህዋሲያን የሚፀዱት?   \n",
      "\n",
      "                                                text  \\\n",
      "0  የአትሌቲክሱ ዓለም ኮከብ እና ፈጣኑ ሰው ዩሴን ቦልት ከውድድር በፊት እና...   \n",
      "1  የስፖርት ጋዜጦች ስለ እግር ኳስ ምን እያሉ ነው? በሚቀጥለው ጥር የሚከፈ...   \n",
      "2  ከኮሮናቫይረስ ወረርሽኝ ተጽእኖ ሳያገግም የዩክሬን እና ሩሲያ ጦርነት የገ...   \n",
      "3  በኮሮናቫይረስ የሞቱት የሮማኒያው ከንቲባ በቅርቡ የተደረገውን ምርጫ በከፍ...   \n",
      "4  የኮሮናቫይረስ ወረርሽኝ መከሰቱን ተከትሎ  ቀጥ ብሎ የነበረውን የአለም የ...   \n",
      "\n",
      "                                       headline_text  \\\n",
      "0  የስፖርት ኮከቦች እና የንግድ ምልክቶቻቸው- ከቦልት እስከ ክርስቲያኖ ሮና...   \n",
      "1  እግር ኳስ፡ ዩናይትድ፣ አርሴናል፣ ቼልሲ . . . ምን አስበዋል? የስፖር...   \n",
      "2  ዓለምን ካስጨነቃት የዋጋ ንረት ተጠቃሚዎቹ እነማን ናቸው? ከኮሮናቫይረስ ...   \n",
      "3  ኮሮናቫይረስ፡ በቫይረሱ የሞቱት የሮማኒያው ከንቲባ በምርጫ አሸነፉ በኮሮና...   \n",
      "4  ኮሮናቫይረስ፡ አውሮፕላኖች እንዴት ነው በፀረ- ተህዋሲያን የሚፀዱት? የኮ...   \n",
      "\n",
      "                                                 url lang  \n",
      "0  https://www.bbc.com/amharic/articles/ceknk30j2xxo  amh  \n",
      "1          https://www.bbc.com/amharic/news-55435608  amh  \n",
      "2  https://www.bbc.com/amharic/articles/crgj31l8mzzo  amh  \n",
      "3          https://www.bbc.com/amharic/news-54336166  amh  \n",
      "4               https://www.bbc.com/amharic/53627279  amh  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\user\\Language-ID\\data\\Language_ID\\masakhanews\\processed\\train.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  \"\"\"cleans text data\"\"\"\n",
    "  if text is None:\n",
    "    return \"\"\n",
    "  \n",
    "  # convert into lowercase\n",
    "  text = text.lower()\n",
    "\n",
    "  #remove urls\n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "  #remove html tags\n",
    "  txt = re.sub(r'<.*?>', '', text)\n",
    "  return text.strip()\n",
    "\n",
    "def load_and_preprocess(processed_path, target_languages):\n",
    "  \"\"\"\n",
    "  load and preprocess data from csvs, uisng the 'text' column\n",
    "  returns:\n",
    "   preprocessed train, val and test data and labes, tokenizer, max length, num classes\n",
    "  \"\"\"\n",
    "  train_df = pd.read_csv(os.path.join(processed_path, \"train.csv\"), usecols=[\"text\", \"lang\"]).dropna()\n",
    "  val_df = pd.read_csv(os.path.join(processed_path, \"validation.csv\"), usecols=[\"text\", \"lang\"]).dropna()\n",
    "  test_df = pd.read_csv(os.path.join(processed_path, \"test.csv\"), usecols=[\"text\", \"lang\"]).dropna()\n",
    "\n",
    "  # apply cleaning to 'text' column\n",
    "  train_df['text'] = train_df['text'].apply(clean_text)\n",
    "  val_df['text'] = val_df['text'].apply(clean_text)\n",
    "  test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "  #prepare text for tokenizer fitting from the 'text' column\n",
    "  all_train_texts = train_df['text'].tolist()\n",
    "\n",
    "  #create and fit tokenizer on training data\n",
    "  tokenizer = Tokenizer(char_level=True, oov_token='<unk>')\n",
    "  tokenizer.fit_on_texts(all_train_texts)\n",
    "\n",
    "  # max sequence length\n",
    "  max_length = min(max([len(text) for text in all_train_texts]), 500)\n",
    "\n",
    "  # convert texts to sequences and pad\n",
    "  X_train = tokenizer.texts_to_sequences(train_df['text'].tolist())\n",
    "  X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "  X_val = tokenizer.texts_to_sequences(val_df['text'].tolist())\n",
    "  X_val = pad_sequences(X_val, maxlen=max_length)\n",
    "\n",
    "  X_test = tokenizer.texts_to_sequences(test_df['text'].tolist())\n",
    "  X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "  # convert language labels into numerical format\n",
    "  language_to_index = {lang: i for i, lang in enumerate(target_languages)}\n",
    "  y_train = to_categorical(train_df['lang'].map(language_to_index), num_classes=len(target_languages))\n",
    "  y_val = to_categorical(val_df['lang'].map(language_to_index), num_classes=len(target_languages))\n",
    "  y_test = to_categorical(test_df['lang'].map(language_to_index), num_classes=len(target_languages))\n",
    "\n",
    "  return (X_train, y_train), (X_val, y_val), (X_test, y_test), tokenizer, max_length, len(target_languages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Architecture & Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_cnn_model(vocab_size, max_length, num_classes, embedding_dim=64):\n",
    "    \"\"\"\n",
    "    Build a CNN model for language identification.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of the vocabulary (number of unique characters)\n",
    "        max_length: Maximum sequence length\n",
    "        num_classes: Number of languages to classify\n",
    "        embedding_dim: Dimension of character embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Character embedding layer\n",
    "        Embedding(input_dim=vocab_size + 1,  # +1 for padding token\n",
    "                 output_dim=embedding_dim,\n",
    "                 input_length=max_length),\n",
    "        \n",
    "        Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adamw',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, batch_size=64, epochs=10, patience=3):\n",
    "    \"\"\"\n",
    "    Train the CNN model with early stopping.\n",
    "    \"\"\"\n",
    "    # Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Language-ID\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 186ms/step - accuracy: 0.2705 - loss: 3.1345 - val_accuracy: 0.5971 - val_loss: 0.9371\n",
      "Epoch 2/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 176ms/step - accuracy: 0.5817 - loss: 0.9866 - val_accuracy: 0.7471 - val_loss: 0.7007\n",
      "Epoch 3/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 157ms/step - accuracy: 0.6672 - loss: 0.7957 - val_accuracy: 0.7048 - val_loss: 0.6529\n",
      "Epoch 4/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - accuracy: 0.7444 - loss: 0.6671 - val_accuracy: 0.8442 - val_loss: 0.5067\n",
      "Epoch 5/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 162ms/step - accuracy: 0.7930 - loss: 0.6148 - val_accuracy: 0.8913 - val_loss: 0.4491\n",
      "Epoch 6/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - accuracy: 0.8468 - loss: 0.5207 - val_accuracy: 0.8885 - val_loss: 0.4070\n",
      "Epoch 7/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 168ms/step - accuracy: 0.8691 - loss: 0.4882 - val_accuracy: 0.9240 - val_loss: 0.3794\n",
      "Epoch 8/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.8812 - loss: 0.4758 - val_accuracy: 0.9567 - val_loss: 0.3304\n",
      "Epoch 9/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 168ms/step - accuracy: 0.9109 - loss: 0.4156 - val_accuracy: 0.9635 - val_loss: 0.3045\n",
      "Epoch 10/10\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 177ms/step - accuracy: 0.9172 - loss: 0.3857 - val_accuracy: 0.9644 - val_loss: 0.2869\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), tokenizer, max_length, num_classes = load_and_preprocess(processed_path, target_languages)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "model = build_cnn_model(vocab_size, max_length, num_classes)\n",
    "\n",
    "model, history = train_model(model, X_train, y_train, X_val, y_val, batch_size=64, epochs=10, patience=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, language_codes):\n",
    "    \"\"\"\n",
    "    Evaluate the model and generate comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        X_test: Test data\n",
    "        y_test: One-hot encoded test labels\n",
    "        language_codes: List of language codes in the same order as labels\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 for each language\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=range(len(language_codes))\n",
    "    )\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Prepare metrics dictionary\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"language_metrics\": {}\n",
    "    }\n",
    "    \n",
    "    # Add per-language metrics\n",
    "    for i, lang in enumerate(language_codes):\n",
    "        metrics[\"language_metrics\"][lang] = {\n",
    "            \"precision\": float(precision[i]),\n",
    "            \"recall\": float(recall[i]),\n",
    "            \"f1_score\": float(f1[i]),\n",
    "            \"support\": int(support[i])\n",
    "        }\n",
    "    \n",
    "    return metrics, cm\n",
    "\n",
    "def save_metrics(metrics, output_dir=\"results\"):\n",
    "    \"\"\"Save metrics to a JSON file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"evaluation_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "def plot_confusion_matrix(cm, language_codes, output_dir=\"results\"):\n",
    "    \"\"\"Plot and save the confusion matrix.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=language_codes, \n",
    "                yticklabels=language_codes)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_history(history, output_dir=\"results\"):\n",
    "    \"\"\"Plot and save the training history.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_history.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "{\n",
      "    \"accuracy\": 0.9702922855773838,\n",
      "    \"language_metrics\": {\n",
      "        \"amh\": {\n",
      "            \"precision\": 1.0,\n",
      "            \"recall\": 0.9973404255319149,\n",
      "            \"f1_score\": 0.9986684420772304,\n",
      "            \"support\": 376\n",
      "        },\n",
      "        \"ibo\": {\n",
      "            \"precision\": 0.9974293059125964,\n",
      "            \"recall\": 0.9948717948717949,\n",
      "            \"f1_score\": 0.9961489088575096,\n",
      "            \"support\": 390\n",
      "        },\n",
      "        \"orm\": {\n",
      "            \"precision\": 0.9587301587301588,\n",
      "            \"recall\": 0.9292307692307692,\n",
      "            \"f1_score\": 0.94375,\n",
      "            \"support\": 325\n",
      "        },\n",
      "        \"pcm\": {\n",
      "            \"precision\": 0.9404388714733543,\n",
      "            \"recall\": 0.9836065573770492,\n",
      "            \"f1_score\": 0.9615384615384616,\n",
      "            \"support\": 305\n",
      "        },\n",
      "        \"run\": {\n",
      "            \"precision\": 0.9161676646706587,\n",
      "            \"recall\": 0.9503105590062112,\n",
      "            \"f1_score\": 0.9329268292682927,\n",
      "            \"support\": 322\n",
      "        },\n",
      "        \"sna\": {\n",
      "            \"precision\": 0.9971830985915493,\n",
      "            \"recall\": 0.959349593495935,\n",
      "            \"f1_score\": 0.9779005524861878,\n",
      "            \"support\": 369\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), tokenizer, max_length, num_classes = load_and_preprocess(\n",
    "  processed_path = r\"C:\\Users\\user\\Language-ID\\data\\Language_ID\\masakhanews\\processed\",\n",
    "  target_languages = [\"amh\", \"ibo\", \"orm\", \"pcm\", \"run\", \"sna\"]\n",
    ")\n",
    "\n",
    "language_codes = [\"amh\", \"ibo\", \"orm\", \"pcm\", \"run\", \"sna\"]\n",
    "\n",
    "# Evaluate the model\n",
    "metrics, cm = evaluate_model(model, X_test, y_test, language_codes)\n",
    "\n",
    "# Save metrics to a JSON file\n",
    "save_metrics(metrics, output_dir=\"results\")\n",
    "\n",
    "# Plot and save confusion matrix\n",
    "plot_confusion_matrix(cm, language_codes, output_dir=\"results\")\n",
    "\n",
    "# Plot and save training history\n",
    "plot_training_history(history, output_dir=\"results\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(json.dumps(metrics, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>በ ብ ቃ ት   የ ማ ይ መ ረ ቱ   ም ር ቶ ች   ላ ይ   ከ ው ...</td>\n",
       "      <td>amh</td>\n",
       "      <td>ibo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>pcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u .   n ' o t u   a   a t l e t i c o   m a d ...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r a   1 9 7 0   k a n   d h a l a t e   d m x ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h a n n a a n   d u b a r t i i   s a n a   w ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>’ t   b a c k   d o w n ’     h a y y a m a   ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>o d a a a n e   -   a k k a   d a n s a a   d ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>r o o   w a l i i n   m e e t i r a   2   a k ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s a g a l e e n   h a t a m e e r a   j e c h ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a n i i f \"   j e d h a n .   s h o r o r k e ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>o o t a   m i i d h a m a a   j i r a n   b i ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>pcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t i g r a a y   d a b a l a t e e   n a m o ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>pcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h a a b u u f i   g u d d i s u u t i .   a k ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>pcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>y o r k   t i m e s   w a l i i n   t u r t ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b s o o   h i d h a n n o o   k a a s e e   h ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i i n d a a   a r d e e n   t i w i i t a r a ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i t t i   j o o r j i   w i h a a n   g a r a ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>g g a r s a   n a a f   t a a s i s a n   h u ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>sna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>o k k o n   i s i n i t t i   h i m a .   s o ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>i f a c h u u f   h o j i i   k a n a   h o j ...</td>\n",
       "      <td>orm</td>\n",
       "      <td>pcm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text True Label  \\\n",
       "0     በ ብ ቃ ት   የ ማ ይ መ ረ ቱ   ም ር ቶ ች   ላ ይ   ከ ው ...        amh   \n",
       "1   <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk...        ibo   \n",
       "2   u .   n ' o t u   a   a t l e t i c o   m a d ...        ibo   \n",
       "3   r a   1 9 7 0   k a n   d h a l a t e   d m x ...        orm   \n",
       "4   h a n n a a n   d u b a r t i i   s a n a   w ...        orm   \n",
       "5   ’ t   b a c k   d o w n ’     h a y y a m a   ...        orm   \n",
       "6   o d a a a n e   -   a k k a   d a n s a a   d ...        orm   \n",
       "7   r o o   w a l i i n   m e e t i r a   2   a k ...        orm   \n",
       "8   s a g a l e e n   h a t a m e e r a   j e c h ...        orm   \n",
       "9   a n i i f \"   j e d h a n .   s h o r o r k e ...        orm   \n",
       "10  o o t a   m i i d h a m a a   j i r a n   b i ...        orm   \n",
       "11    t i g r a a y   d a b a l a t e e   n a m o ...        orm   \n",
       "12  h a a b u u f i   g u d d i s u u t i .   a k ...        orm   \n",
       "13    y o r k   t i m e s   w a l i i n   t u r t ...        orm   \n",
       "14  b s o o   h i d h a n n o o   k a a s e e   h ...        orm   \n",
       "15  i i n d a a   a r d e e n   t i w i i t a r a ...        orm   \n",
       "16  i t t i   j o o r j i   w i h a a n   g a r a ...        orm   \n",
       "17  g g a r s a   n a a f   t a a s i s a n   h u ...        orm   \n",
       "18  o k k o n   i s i n i t t i   h i m a .   s o ...        orm   \n",
       "19  i f a c h u u f   h o j i i   k a n a   h o j ...        orm   \n",
       "\n",
       "   Predicted Label  \n",
       "0              ibo  \n",
       "1              pcm  \n",
       "2              run  \n",
       "3              run  \n",
       "4              run  \n",
       "5              run  \n",
       "6              run  \n",
       "7              run  \n",
       "8              run  \n",
       "9              run  \n",
       "10             pcm  \n",
       "11             pcm  \n",
       "12             pcm  \n",
       "13             run  \n",
       "14             run  \n",
       "15             run  \n",
       "16             run  \n",
       "17             sna  \n",
       "18             run  \n",
       "19             pcm  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get misclassified samples\n",
    "misclassified = X_test[np.argmax(y_test, axis=1) != np.argmax(model.predict(X_test), axis=1)]\n",
    "misclassified_labels = np.argmax(y_test, axis=1)[np.argmax(y_test, axis=1) != np.argmax(model.predict(X_test), axis=1)]\n",
    "misclassified_preds = np.argmax(model.predict(X_test), axis=1)[np.argmax(y_test, axis=1) != np.argmax(model.predict(X_test), axis=1)]\n",
    "\n",
    "# Create a DataFrame for easy analysis\n",
    "df = pd.DataFrame({\n",
    "    \"Text\": tokenizer.sequences_to_texts(misclassified),\n",
    "    \"True Label\": [language_codes[i] for i in misclassified_labels],\n",
    "    \"Predicted Label\": [language_codes[i] for i in misclassified_preds]\n",
    "})\n",
    "\n",
    "df.head(20)  # Show first 20 misclassified examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, tokenizer, max_length, language_codes, output_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Save the trained model and necessary metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        tokenizer: Fitted tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "        language_codes: List of language codes\n",
    "        output_dir: Directory to save model\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(output_dir, \"language_identification_model.h5\")\n",
    "    model.save(model_path)\n",
    "    \n",
    "    # Save tokenizer vocabulary\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with open(os.path.join(output_dir, \"tokenizer.json\"), \"w\") as f:\n",
    "        f.write(tokenizer_json)\n",
    "    \n",
    "    # Save metadata (max_length, language codes)\n",
    "    metadata = {\n",
    "        \"max_length\": max_length,\n",
    "        \"language_codes\": language_codes\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Tokenizer and metadata saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\language_identification_model.h5\n",
      "Tokenizer and metadata saved to models\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, max_length, language_codes, output_dir=\"models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
